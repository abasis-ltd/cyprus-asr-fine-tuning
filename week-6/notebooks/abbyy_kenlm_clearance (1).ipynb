{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import regex\n",
        "import os"
      ],
      "metadata": {
        "id": "3oZTGphDRYN5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clear_path = './ABBYY_clear.txt'\n",
        "edited_path = './ABBYY_edited.txt'"
      ],
      "metadata": {
        "id": "e8BSfr1oRbZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(clear_path, 'r', encoding='utf-8') as f:\n",
        "    clear = f.read()"
      ],
      "metadata": {
        "id": "xKt4AM_ZRwb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(edited_path, 'r', encoding='utf-8') as f:\n",
        "    edited = f.read()"
      ],
      "metadata": {
        "id": "41YdGBOpSJp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_clear = regex.sub(r\"[^\\p{L} ]+\", \"\", clear.lower())\n",
        "edited_clear = regex.sub(r\"[^\\p{L} ]+\", \"\", edited.lower())"
      ],
      "metadata": {
        "id": "sEVIYPzCSwhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_clean = clean_clear.split()\n",
        "words_edited = edited_clear.split()"
      ],
      "metadata": {
        "id": "6g5JyO2sT7mT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_clean_combined = []\n",
        "edited_clean_combined = []"
      ],
      "metadata": {
        "id": "Uu7zjd2sU9AJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split(words, combined):\n",
        "  for i in range(0, len(words), 10):\n",
        "      chunk = words[i:i+10]\n",
        "      line = \" \".join(chunk)\n",
        "      combined.append(line)\n",
        "  return combined"
      ],
      "metadata": {
        "id": "QGrJ5YZuUO2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split(words_clean, words_clean_combined);\n",
        "split(words_edited, edited_clean_combined);"
      ],
      "metadata": {
        "id": "xSfUwMAeVaBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kenlm_combined = []"
      ],
      "metadata": {
        "id": "dsJyirKySb44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def append_ken(kenlm, choosen):\n",
        "    for sentence in choosen:\n",
        "      kenlm.append(sentence)\n",
        "    return kenlm"
      ],
      "metadata": {
        "id": "ahK8dd-FVpr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# append_ken(kenlm_combined, words_clean_combined);\n",
        "# append_ken(kenlm_combined, edited_clean_combined);"
      ],
      "metadata": {
        "id": "JzIqnLSSVre7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in kenlm_combined:\n",
        "        f.write(line + \"\\n\")"
      ],
      "metadata": {
        "id": "AZ2l5SlhWN8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_kenlm_corpus(input_filename, output_filename):\n",
        "    with open(input_filename, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # Шаг 1: Удаление этимологических маркеров и сокращений\n",
        "    regex_to_remove = re.compile(\n",
        "        r'\\b(α|β|γ|δ|ε|η|θ|ι|κ|λ|μ|ν|ξ|ο|π|ρ|σ|τ|υ|φ|χ|ψ|ω|αόρ|αμ|ανθ|επίθ|επίρρ|μτφ|συνών|αντίθ|φρ|κν|αραβ|γαλλ|ενετ|ιταλ|λατ|τουρκ|θρκ|κερ|μκερ|μαπολλ|μτγν|πρβ|αί|αΐιΐηι|ΐ|ΐί|τ|ΐίηιιηι|ϊίΐι|ίϊ|ως|ώσπερ)\\b',\n",
        "        re.IGNORECASE | re.UNICODE\n",
        "    )\n",
        "    cleaned_text = regex_to_remove.sub('', text)\n",
        "\n",
        "    # Шаг 2: Замена специальных символов и пунктуации на пробелы\n",
        "    cleaned_text = re.sub(r'[^\\s\\w\\d\\u0386-\\u03ce]', ' ', cleaned_text, flags=re.UNICODE)\n",
        "\n",
        "    # Шаг 3: Удаление лишних пробелов и символов в начале/конце строк\n",
        "    lines = cleaned_text.split('\\n')\n",
        "    cleaned_lines = [re.sub(r'\\s+', ' ', line).strip() for line in lines if line.strip()]\n",
        "\n",
        "    final_text = '\\n'.join(cleaned_lines)\n",
        "\n",
        "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "        f.write(final_text)\n",
        "\n",
        "# Укажите имя вашего исходного файла и имя файла для результата\n",
        "input_file = './KENLM_ABBYY.txt'\n",
        "output_file = 'cleaned_for_kenlm.txt'\n",
        "\n",
        "clean_kenlm_corpus(input_file, output_file)\n",
        "print(f\"Файл '{input_file}' успешно очищен. Результат сохранен в '{output_file}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wI0k5JoTfrdp",
        "outputId": "5cfabc43-96db-4028-b50d-c819d15f0978"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Файл './KENLM_ABBYY.txt' успешно очищен. Результат сохранен в 'cleaned_for_kenlm.txt'.\n"
          ]
        }
      ]
    }
  ]
}