{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_04UN3_Y3cU"
      },
      "outputs": [],
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Elormiden/bert-base-cypriot-greek\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"Elormiden/bert-base-cypriot-greek\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "his_tokenizer = AutoTokenizer.from_pretrained(\"petros/bert-base-cypriot-uncased-v1\")\n",
        "his_model = AutoModelForMaskedLM.from_pretrained(\"petros/bert-base-cypriot-uncased-v1\")"
      ],
      "metadata": {
        "id": "POU_q9wUY__r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install jiwer"
      ],
      "metadata": {
        "id": "BRBBk8_87RqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import torch\n",
        "import jiwer\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from jiwer import wer, cer"
      ],
      "metadata": {
        "id": "C7QkB36wZ4Xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"Elormiden/Thesaurus-Cypriot-Greek-Dialect\")"
      ],
      "metadata": {
        "id": "RYhY1E8b7AgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class JiwerMetricsPipeline:\n",
        "  def __init__(self, hypothesis: str, truth: str):\n",
        "    self.hypothesis = hypothesis\n",
        "    self.truth = truth\n",
        "\n",
        "  def compute_metrics(self) -> tuple[float, float]:\n",
        "    transformation = jiwer.Compose([\n",
        "        jiwer.ToLowerCase(),\n",
        "        jiwer.RemovePunctuation(),\n",
        "        jiwer.RemoveMultipleSpaces(),\n",
        "        jiwer.Strip()\n",
        "    ])\n",
        "\n",
        "    normalized_truth = transformation(self.truth)\n",
        "    normalized_hypothesis = transformation(self.hypothesis)\n",
        "\n",
        "    wer_score = wer(normalized_truth, normalized_hypothesis)\n",
        "    cer_score = cer(normalized_truth, normalized_hypothesis)\n",
        "\n",
        "    return wer_score, cer_score"
      ],
      "metadata": {
        "id": "EhSHJRQmZ4lA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mlm_top1_prediction(model, tokenizer, text):\n",
        "    \"\"\"\n",
        "    Given a text with a [MASK] token, returns the top 1 predicted token as a string.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "    # Check if mask token exists in input_ids\n",
        "    mask_token_indices = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
        "    if mask_token_indices.numel() == 0:\n",
        "        # No mask token found, return empty string or raise error\n",
        "        # For this specific task, we expect a mask, so we can return an empty string to avoid crashes\n",
        "        return \"\"\n",
        "\n",
        "    mask_token_index = mask_token_indices[0] # Get the first mask token index\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "\n",
        "    mask_token_logits = logits[0, mask_token_index, :]\n",
        "    top_1_token_id = torch.topk(mask_token_logits, 1, dim=-1).indices.squeeze().item()\n",
        "\n",
        "    predicted_token = tokenizer.decode([top_1_token_id]).strip()\n",
        "\n",
        "    return predicted_token"
      ],
      "metadata": {
        "id": "Bc7nv3hJaYDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Συγκεντρωτικοί Πίνακες για τα Αποτελέσματα ---\n",
        "elormiden_cypriot_wer_scores = []\n",
        "elormiden_cypriot_cer_scores = []\n",
        "elormiden_greek_wer_scores = []\n",
        "elormiden_greek_cer_scores = []\n",
        "\n",
        "petros_cypriot_wer_scores = []\n",
        "petros_cypriot_cer_scores = []\n",
        "petros_greek_wer_scores = []\n",
        "petros_greek_cer_scores = []"
      ],
      "metadata": {
        "id": "Tvzo91CxacNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_mlm_with_thesaurus(\n",
        "    elormiden_model, elormiden_tokenizer,\n",
        "    petros_model, petros_tokenizer,\n",
        "    dataset_name=\"Elormiden/Thesaurus-Cypriot-Greek-Dialect\",\n",
        "    split=\"test\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Оценивает две модели MLM (Elormiden и Petros) на основе тезауруса кипрско-греческого диалекта,\n",
        "    используя WER и CER для измерения точности предсказаний.\n",
        "\n",
        "    Args:\n",
        "        elormiden_model: Модель AutoModelForMaskedLM для Elormiden.\n",
        "        elormiden_tokenizer: Токенизатор AutoTokenizer для Elormiden.\n",
        "        petros_model: Модель AutoModelForMaskedLM для Petros.\n",
        "        petros_tokenizer: Токенизатор AutoTokenizer для Petros.\n",
        "        dataset_name (str): Название набора данных для загрузки.\n",
        "        split (str): Разделение набора данных (например, 'train', 'validation').\n",
        "\n",
        "    Returns:\n",
        "        dict: Словарь, содержащий средние WER и CER для каждой модели\n",
        "              по предсказанию кипрских и стандартных греческих слов.\n",
        "    \"\"\"\n",
        "    print(f\"Загрузка данных из '{dataset_name}' (сплит: '{split}')...\")\n",
        "    ds = load_dataset(dataset_name)\n",
        "    print(\"Данные успешно загружены.\")\n",
        "\n",
        "    elormiden_cypriot_wer_scores = []\n",
        "    elormiden_cypriot_cer_scores = []\n",
        "    elormiden_greek_wer_scores = []\n",
        "    elormiden_greek_cer_scores = []\n",
        "\n",
        "    petros_cypriot_wer_scores = []\n",
        "    petros_cypriot_cer_scores = []\n",
        "    petros_greek_wer_scores = []\n",
        "    petros_greek_cer_scores = []\n",
        "\n",
        "    cypriot_template = \"Η λέξη {} είναι από την κυπριακή διάλεκτο.\" # \"Слово {} из кипрского диалекта.\"\n",
        "    standard_greek_template = \"Η λέξη {} χρησιμοποιείται στα ελληνικά.\" # \"Слово {} используется в греческом языке.\"\n",
        "\n",
        "    print(\"\\nНачало оценки моделей...\")\n",
        "    # Используем tqdm для отображения прогресса по всем записям в наборе данных\n",
        "    for entry in tqdm(ds[split], desc=\"Оценка записей\"):\n",
        "        cypriot_word = entry['word']\n",
        "        standard_greek_word = entry['greek_word']\n",
        "\n",
        "        if not cypriot_word or not cypriot_word.strip() or not standard_greek_word or not standard_greek_word.strip():\n",
        "            continue\n",
        "\n",
        "        # --- Оценка модели \"Elormiden/bert-base-cypriot-greek\" ---\n",
        "        masked_cypriot_sentence_elormiden = cypriot_template.format(elormiden_tokenizer.mask_token)\n",
        "        elormiden_cypriot_hypothesis = get_mlm_top1_prediction(elormiden_model, elormiden_tokenizer, masked_cypriot_sentence_elormiden)\n",
        "\n",
        "        metrics_elormiden_cypriot = JiwerMetricsPipeline(elormiden_cypriot_hypothesis, cypriot_word)\n",
        "        wer_score, cer_score = metrics_elormiden_cypriot.compute_metrics()\n",
        "        elormiden_cypriot_wer_scores.append(wer_score)\n",
        "        elormiden_cypriot_cer_scores.append(cer_score)\n",
        "\n",
        "        masked_standard_greek_sentence_elormiden = standard_greek_template.format(elormiden_tokenizer.mask_token)\n",
        "        elormiden_greek_hypothesis = get_mlm_top1_prediction(elormiden_model, elormiden_tokenizer, masked_standard_greek_sentence_elormiden)\n",
        "\n",
        "        metrics_elormiden_greek = JiwerMetricsPipeline(elormiden_greek_hypothesis, standard_greek_word)\n",
        "        wer_score, cer_score = metrics_elormiden_greek.compute_metrics()\n",
        "        elormiden_greek_wer_scores.append(wer_score)\n",
        "        elormiden_greek_cer_scores.append(cer_score)\n",
        "\n",
        "        # --- Оценка модели \"petros/bert-base-cypriot-uncased-v1\" ---\n",
        "        masked_cypriot_sentence_petros = cypriot_template.format(petros_tokenizer.mask_token)\n",
        "        petros_cypriot_hypothesis = get_mlm_top1_prediction(petros_model, petros_tokenizer, masked_cypriot_sentence_petros)\n",
        "\n",
        "        metrics_petros_cypriot = JiwerMetricsPipeline(petros_cypriot_hypothesis, cypriot_word)\n",
        "        wer_score, cer_score = metrics_petros_cypriot.compute_metrics()\n",
        "        petros_cypriot_wer_scores.append(wer_score)\n",
        "        petros_cypriot_cer_scores.append(cer_score)\n",
        "\n",
        "        masked_standard_greek_sentence_petros = standard_greek_template.format(petros_tokenizer.mask_token)\n",
        "        petros_greek_hypothesis = get_mlm_top1_prediction(petros_model, petros_tokenizer, masked_standard_greek_sentence_petros)\n",
        "\n",
        "        metrics_petros_greek = JiwerMetricsPipeline(petros_greek_hypothesis, standard_greek_word)\n",
        "        wer_score, cer_score = metrics_petros_greek.compute_metrics()\n",
        "        petros_greek_wer_scores.append(wer_score)\n",
        "        petros_greek_cer_scores.append(cer_score)\n",
        "\n",
        "    results = {\n",
        "        \"Elormiden_Cypriot_WER\": np.mean(elormiden_cypriot_wer_scores),\n",
        "        \"Elormiden_Cypriot_CER\": np.mean(elormiden_cypriot_cer_scores),\n",
        "        \"Elormiden_StandardGreek_WER\": np.mean(elormiden_greek_wer_scores),\n",
        "        \"Elormiden_StandardGreek_CER\": np.mean(elormiden_greek_cer_scores),\n",
        "        \"Petros_Cypriot_WER\": np.mean(petros_cypriot_wer_scores),\n",
        "        \"Petros_Cypriot_CER\": np.mean(petros_cypriot_cer_scores),\n",
        "        \"Petros_StandardGreek_WER\": np.mean(petros_greek_wer_scores),\n",
        "        \"Petros_StandardGreek_CER\": np.mean(petros_greek_cer_scores),\n",
        "    }\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "JNefK_HUaeps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_results = evaluate_mlm_with_thesaurus(\n",
        "        elormiden_model=model,\n",
        "        elormiden_tokenizer=tokenizer,\n",
        "        petros_model=his_model,\n",
        "        petros_tokenizer=his_tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "3dOI5oltapMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Note:\n",
        "#WER, CER approach does not work for either models, because models don't give the exact transcription, only simmilar ones\n",
        "\n",
        "\"\"\"\n",
        "--- Средние значения WER/CER (чем ниже, тем лучше) ---\n",
        "\n",
        "Модель: Elormiden/bert-base-cypriot-greek\n",
        "  Предсказание кипрского слова - Средний WER: 0.9983, Средний CER: 0.9034\n",
        "  Предсказание стандартного греческого слова - Средний WER: 1.0000, Средний CER: 1.0910\n",
        "\n",
        "Модель: petros/bert-base-cypriot-uncased-v1\n",
        "  Предсказание кипрского слова - Средний WER: 1.0000, Средний CER: 0.8959\n",
        "  Предсказание стандартного греческого слова - Средний WER: 0.9995, Средний CER: 0.9155\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "nMn7-kCEi0lT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "eI7lSfsekYLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Вспомогательная функция для MLM предсказаний (обновлена для возврата топ-K) ---\n",
        "def get_mlm_top_k_predictions(model, tokenizer, text, top_k=5):\n",
        "    \"\"\"\n",
        "    Дано предложение с токеном [MASK], возвращает топ-K предсказанных токенов и их оценки.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "    mask_token_indices = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
        "    if mask_token_indices.numel() == 0:\n",
        "        return [] # Возвращаем пустой список, если маска не найдена\n",
        "\n",
        "    mask_token_index = mask_token_indices[0]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "\n",
        "    mask_token_logits = logits[0, mask_token_index, :]\n",
        "    top_k_results = torch.topk(mask_token_logits, top_k, dim=-1)\n",
        "\n",
        "    predicted_tokens_info = []\n",
        "    for score, token_id in zip(top_k_results.values, top_k_results.indices):\n",
        "        token = tokenizer.decode([token_id]).strip()\n",
        "        predicted_tokens_info.append((token, score.item()))\n",
        "\n",
        "    return predicted_tokens_info\n"
      ],
      "metadata": {
        "id": "LSAy4cb2kead"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- New Helper Function for Random Masking ---\n",
        "def mask_random_word(text, mask_token, min_words=3):\n",
        "    \"\"\"\n",
        "    Masks a random word in the given text.\n",
        "    Returns (masked_text, original_masked_word).\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    if len(words) < min_words: # Ensure enough words to mask meaningfully\n",
        "        return None, None\n",
        "\n",
        "    # Try to avoid masking very short or common words like articles/prepositions\n",
        "    # This is a simple heuristic; more sophisticated methods could be used.\n",
        "    maskable_indices = [\n",
        "        i for i, word in enumerate(words)\n",
        "        if len(word.strip(\".,;!?'\\\"\")) > 2 and word.lower() not in ['ο', 'η', 'το', 'του', 'της', 'των', 'και', 'με', 'για', 'από', 'σε', 'ένα', 'μια', 'ένας']\n",
        "    ]\n",
        "\n",
        "    if not maskable_indices: # If no suitable words found, just pick a random one\n",
        "        mask_idx = random.randint(0, len(words) - 1)\n",
        "    else:\n",
        "        mask_idx = random.choice(maskable_indices)\n",
        "\n",
        "    original_masked_word = words[mask_idx].strip(\".,;!?'\\\"\").lower() # Store original, normalized\n",
        "    words[mask_idx] = mask_token # Replace with mask token\n",
        "    masked_text = \" \".join(words)\n",
        "    return masked_text, original_masked_word"
      ],
      "metadata": {
        "id": "EPzMP8plmGTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- REVISED analyze_mlm_predictions_qualitatively Function ---\n",
        "def analyze_mlm_predictions_qualitatively(\n",
        "    elormiden_model, elormiden_tokenizer,\n",
        "    petros_model, petros_tokenizer,\n",
        "    dataset_name=\"Elormiden/Thesaurus-Cypriot-Greek-Dialect\",\n",
        "    split=\"train\",\n",
        "    num_examples=10, # More examples by default for better qualitative feel\n",
        "    top_k=10 # Show more top predictions\n",
        "):\n",
        "    \"\"\"\n",
        "    Performs qualitative analysis of MLM predictions by selecting random examples\n",
        "    from the thesaurus's descriptions and masking a random word within them.\n",
        "    Displays top-K predictions for each model in different contexts.\n",
        "    \"\"\"\n",
        "    print(f\"\\nЗагрузка данных для качественного анализа из '{dataset_name}' (сплит: '{split}')...\")\n",
        "    ds = load_dataset(dataset_name)\n",
        "    print(\"Данные успешно загружены.\")\n",
        "\n",
        "    # Filter for valid entries with non-empty descriptions\n",
        "    valid_entries = [\n",
        "        entry for entry in ds[split]\n",
        "        if entry['description'] and entry['description'].strip() and \\\n",
        "           entry['greek_description'] and entry['greek_description'].strip()\n",
        "    ]\n",
        "    random_examples = random.sample(valid_entries, min(num_examples, len(valid_entries)))\n",
        "\n",
        "    print(f\"\\n--- Качественный Анализ Топ-{top_k} Предсказаний (с маскировкой описаний) для {num_examples} Случайных Примеров ---\")\n",
        "\n",
        "    for i, entry in enumerate(random_examples):\n",
        "        cypriot_desc = entry['description']\n",
        "        greek_desc = entry['greek_description']\n",
        "        cypriot_word = entry['word'] # Keep for reference, even if not masked directly\n",
        "        greek_word = entry['greek_word'] # Keep for reference\n",
        "\n",
        "        print(f\"\\n=== Пример {i+1} ===\")\n",
        "        print(f\"  > Оригинальное кипрское слово: '{cypriot_word}'\")\n",
        "        print(f\"  > Оригинальное стандартное греческое слово: '{greek_word}'\")\n",
        "        print(f\"  > Истинное кипрское описание: '{cypriot_desc}'\")\n",
        "        print(f\"  > Истинное стандартное греческое описание: '{greek_desc}'\")\n",
        "\n",
        "        # --- Elormiden Model ---\n",
        "        print(\"\\n  **Модель: Elormiden/bert-base-cypriot-greek**\")\n",
        "\n",
        "        # Cypriot Description Context\n",
        "        masked_cypriot_desc_elormiden, original_masked_cypriot_word_elormiden = mask_random_word(cypriot_desc, elormiden_tokenizer.mask_token)\n",
        "        if masked_cypriot_desc_elormiden:\n",
        "            elormiden_cypriot_preds = get_mlm_top_k_predictions(elormiden_model, elormiden_tokenizer, masked_cypriot_desc_elormiden, top_k)\n",
        "            print(f\"    Предсказания для КИПРСКОГО описания (Оригинал: '{cypriot_desc}', Маска: '{masked_cypriot_desc_elormiden}', Истинное замаскированное: '{original_masked_cypriot_word_elormiden}'):\")\n",
        "            for rank, (token, score) in enumerate(elormiden_cypriot_preds):\n",
        "                print(f\"      {rank+1}. '{token}' (Score: {score:.4f})\")\n",
        "        else:\n",
        "            print(\"    Недостаточно слов для маскировки в кипрском описании.\")\n",
        "\n",
        "        # Standard Greek Description Context\n",
        "        masked_greek_desc_elormiden, original_masked_greek_word_elormiden = mask_random_word(greek_desc, elormiden_tokenizer.mask_token)\n",
        "        if masked_greek_desc_elormiden:\n",
        "            elormiden_greek_preds = get_mlm_top_k_predictions(elormiden_model, elormiden_tokenizer, masked_greek_desc_elormiden, top_k)\n",
        "            print(f\"    Предсказания для СТАНДАРТНОГО ГРЕЧЕСКОГО описания (Оригинал: '{greek_desc}', Маска: '{masked_greek_desc_elormiden}', Истинное замаскированное: '{original_masked_greek_word_elormiden}'):\")\n",
        "            for rank, (token, score) in enumerate(elormiden_greek_preds):\n",
        "                print(f\"      {rank+1}. '{token}' (Score: {score:.4f})\")\n",
        "        else:\n",
        "            print(\"    Недостаточно слов для маскировки в стандартном греческом описании.\")\n",
        "\n",
        "        # --- Petros Model ---\n",
        "        print(\"\\n  **Модель: petros/bert-base-cypriot-uncased-v1**\")\n",
        "\n",
        "        # Cypriot Description Context\n",
        "        masked_cypriot_desc_petros, original_masked_cypriot_word_petros = mask_random_word(cypriot_desc, petros_tokenizer.mask_token)\n",
        "        if masked_cypriot_desc_petros:\n",
        "            petros_cypriot_preds = get_mlm_top_k_predictions(petros_model, petros_tokenizer, masked_cypriot_desc_petros, top_k)\n",
        "            print(f\"    Предсказания для КИПРСКОГО описания (Оригинал: '{cypriot_desc}', Маска: '{masked_cypriot_desc_petros}', Истинное замаскированное: '{original_masked_cypriot_word_petros}'):\")\n",
        "            for rank, (token, score) in enumerate(petros_cypriot_preds):\n",
        "                print(f\"      {rank+1}. '{token}' (Score: {score:.4f})\")\n",
        "        else:\n",
        "            print(\"    Недостаточно слов для маскировки в кипрском описании.\")\n",
        "\n",
        "        # Standard Greek Description Context\n",
        "        masked_greek_desc_petros, original_masked_greek_word_petros = mask_random_word(greek_desc, petros_tokenizer.mask_token)\n",
        "        if masked_greek_desc_petros:\n",
        "            petros_greek_preds = get_mlm_top_k_predictions(petros_model, petros_tokenizer, masked_greek_desc_petros, top_k)\n",
        "            print(f\"    Предсказания для СТАНДАРТНОГО ГРЕЧЕСКОГО описания (Оригинал: '{greek_desc}', Маска: '{masked_greek_desc_petros}', Истинное замаскированное: '{original_masked_greek_word_petros}'):\")\n",
        "            for rank, (token, score) in enumerate(petros_greek_preds):\n",
        "                print(f\"      {rank+1}. '{token}' (Score: {score:.4f})\")\n",
        "        else:\n",
        "            print(\"    Недостаточно слов для маскировки в стандартном греческом описании.\")\n",
        "\n",
        "        print(\"-\" * 50) # Separator for readability"
      ],
      "metadata": {
        "id": "JuuvZ_oJkQu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_mlm_predictions_qualitatively(\n",
        "        elormiden_model=model,\n",
        "        elormiden_tokenizer=tokenizer,\n",
        "        petros_model=his_model,\n",
        "        petros_tokenizer=his_tokenizer,\n",
        "        num_examples=10,\n",
        "        top_k=10\n",
        "    )"
      ],
      "metadata": {
        "id": "XlnayVDHkRSg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}