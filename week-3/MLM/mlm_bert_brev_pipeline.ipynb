{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e1ca2e4f-dc24-4329-a3d9-44bf977e8763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets==3.6.0\n",
    "# !pip install transformers\n",
    "# !pip install tf-keras\n",
    "# !pip install transformers[torch]\n",
    "# !pip install wandb\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7ae618dc-2bc4-42d8-bb70-6ca91d4276c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger(\"pyngrok\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"torch\").setLevel(logging.ERROR)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0d456aa5-b615-4304-9e9a-49838ba98b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "PyTorch version: 2.7.0\n",
      "CUDA device: NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "93f497bf-2bd8-4fb8-b0d4-f3944a8c5f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8078d438-f3d4-4c91-b9ca-46e7ac479c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-multilingual-cased\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"google-bert/bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f3ea4db2-7fb0-4e94-a0ea-b3f8cbee0259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, Audio\n",
    "\n",
    "ds_cy = load_dataset(\"Elormiden/Thesaurus-Cypriot-Greek-Dialect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0a008e93-1735-4dfe-a907-424c1abefc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dividing our initial dataset by columns\n",
    "\"\"\"\n",
    "train_cy = ds_cy['train']\n",
    "val_cy = ds_cy['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "80d4aa61-e08f-44ae-b685-114a989db15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_for_mlm(batch):\n",
    "    tokenized_texts = []\n",
    "    \n",
    "    for word, description, greek_word, greek_desc in tqdm(\n",
    "        zip(batch['word'], batch['description'], batch['greek_word'], batch['greek_description']),\n",
    "        total=len(batch['word']), desc=\"Tokenizing for MLM\"):\n",
    "        \n",
    "        full_text = f\"{word} - {description} [SEP] {greek_word} - {greek_desc}\"\n",
    "        tokenized_texts.append(tokenizer.encode(\n",
    "            full_text, \n",
    "            add_special_tokens=True,\n",
    "            max_length=512,           # ← 512 is the limit\n",
    "            truncation=True          \n",
    "            )\n",
    "        )\n",
    "    \n",
    "    input_ids_tensors = [torch.tensor(ids, dtype=torch.long) for ids in tokenized_texts]\n",
    "    input_ids_padded = pad_sequence(input_ids_tensors, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    \n",
    "    input_ids_masked, labels = mask_tokens(input_ids_padded, tokenizer, mlm_probability=0.15)\n",
    "    attention_mask = (input_ids_masked != tokenizer.pad_token_id).long()\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids_masked,  # С [MASK] токенам\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "def mask_tokens(inputs, tokenizer, mlm_probability=0.15):\n",
    "    labels = inputs.clone()\n",
    "\n",
    "    probability_matrix = torch.full(labels.shape, mlm_probability)\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    \n",
    "    special_tokens_mask = torch.zeros_like(labels, dtype=torch.bool)\n",
    "    for token_id in [tokenizer.pad_token_id, tokenizer.cls_token_id, tokenizer.sep_token_id]:\n",
    "        special_tokens_mask |= (labels == token_id)\n",
    "    \n",
    "    masked_indices &= ~special_tokens_mask\n",
    "    \n",
    "    labels[~masked_indices] = -100\n",
    "    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "    inputs[indices_replaced] = tokenizer.mask_token_id\n",
    "    \n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "be68d513-81c7-4fb2-a376-3956f799b3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing for MLM: 100%|██████████| 21971/21971 [00:05<00:00, 4142.16it/s]\n",
      "Tokenizing for MLM: 100%|██████████| 2746/2746 [00:00<00:00, 4213.11it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Filling into the function\n",
    "\"\"\"\n",
    "train_cyprus_tokenized = tokenize_for_mlm(train_cy)\n",
    "val_cyprus_tokenized = tokenize_for_mlm(val_cy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9225b23b-a561-40ca-89d0-caf12a0ec71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Converting dicts to Dataset HuggingFace format\n",
    "\"\"\"\n",
    "train_hf = Dataset.from_dict(train_cyprus_tokenized)\n",
    "val_hf = Dataset.from_dict(val_cyprus_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e48720e5-6bf2-4ec9-b72c-e039a57d842e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./cypriot-corrector-bert-mlm-lr5e-5-batch16\",\n",
    "    num_train_epochs=8,\n",
    "    \n",
    "    ################# \n",
    "    per_device_train_batch_size=16,        \n",
    "    per_device_eval_batch_size=16,         \n",
    "    gradient_accumulation_steps=1,       \n",
    "    ################\n",
    "    \n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=1000,\n",
    "    \n",
    "    #################### A100 \n",
    "    gradient_checkpointing=False,        \n",
    "    bf16=True,                           \n",
    "    dataloader_pin_memory=True,        \n",
    "    dataloader_num_workers=4,            \n",
    "    #################\n",
    "    \n",
    "    save_steps=200,\n",
    "    eval_steps=50,                      \n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to='wandb',\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    logging_steps=50,                    \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_hf,\n",
    "    eval_dataset=val_hf,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2c80f0e8-7ec8-44ed-8f99-4f8187ad1d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "with zipfile.ZipFile('cypriot_bert_best_model.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./best_one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f9dfb2d2-6630-4cea-af20-d63f18b6bc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "new_tokenizer = AutoTokenizer.from_pretrained(\"./best_one\")\n",
    "new_model = AutoModelForMaskedLM.from_pretrained(\"./best_one\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
