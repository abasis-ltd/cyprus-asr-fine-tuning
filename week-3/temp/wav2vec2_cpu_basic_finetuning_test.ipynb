{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers datasets torchaudio jiwer\n",
        "# !pip install datasets==3.6.0"
      ],
      "metadata": {
        "id": "7tfWRVoBa54s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwQgpXD-awSi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset, Audio\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, Trainer, TrainingArguments\n",
        "\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(\"\")"
      ],
      "metadata": {
        "id": "6bUESvLLfigw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "dataset = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"el\", split=\"train\")\n",
        "test_dataset = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"el\", split=\"test\")"
      ],
      "metadata": {
        "id": "F2c19VQDfPdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "test_dataset = test_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
      ],
      "metadata": {
        "id": "ijm8rer4iCbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.column_names"
      ],
      "metadata": {
        "id": "x1pu8vkxkcF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "MODEL_ID = \"jonatasgrosman/wav2vec2-large-xlsr-53-greek\"\n",
        "processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\n",
        "model = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)\n",
        "def preprocess(batch):\n",
        "    audio = batch[\"audio\"]\n",
        "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
        "    batch[\"labels\"] = processor.tokenizer(batch[\"sentence\"]).input_ids\n",
        "    return batch"
      ],
      "metadata": {
        "id": "5QAE0B7PfTkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_remove = [col for col in dataset.column_names]"
      ],
      "metadata": {
        "id": "2q7XiehajLj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(preprocess, remove_columns=columns_to_remove)\n",
        "test_dataset = test_dataset.map(preprocess, remove_columns=columns_to_remove)"
      ],
      "metadata": {
        "id": "0gKyI2y5fbsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)\n",
        "print(test_dataset)"
      ],
      "metadata": {
        "id": "tzOxkioGjRgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorCTCWithPadding:\n",
        "    \"\"\"\n",
        "    Data collator that will dynamically pad the inputs received.\n",
        "    Args:\n",
        "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
        "            The processor used for proccessing the data.\n",
        "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
        "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
        "            among:\n",
        "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
        "              sequence if provided).\n",
        "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
        "              maximum acceptable input length for the model if that argument is not provided.\n",
        "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
        "              different lengths).\n",
        "        max_length (:obj:`int`, `optional`):\n",
        "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
        "        max_length_labels (:obj:`int`, `optional`):\n",
        "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
        "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
        "            If set will pad the sequence to a multiple of the provided value.\n",
        "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
        "            7.5 (Volta).\n",
        "    \"\"\"\n",
        "\n",
        "    processor: Wav2Vec2Processor\n",
        "    padding: Union[bool, str] = True\n",
        "    max_length: Optional[int] = None\n",
        "    max_length_labels: Optional[int] = None\n",
        "    pad_to_multiple_of: Optional[int] = None\n",
        "    pad_to_multiple_of_labels: Optional[int] = None\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need\n",
        "        # different padding methods\n",
        "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        batch = self.processor.pad(\n",
        "            input_features,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        with self.processor.as_target_processor():\n",
        "            labels_batch = self.processor.pad(\n",
        "                label_features,\n",
        "                padding=self.padding,\n",
        "                max_length=self.max_length_labels,\n",
        "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n"
      ],
      "metadata": {
        "id": "M9UnT1WNlFBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define metrics\n",
        "# wer_metric = load_metric(\"wer\")\n",
        "# def compute_metrics(pred):\n",
        "#     pred_logits = pred.predictions\n",
        "#     pred_ids = torch.argmax(torch.tensor(pred_logits), dim=-1)\n",
        "#     pred_str = processor.batch_decode(pred_ids)\n",
        "#     label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
        "#     wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
        "#     return {\"wer\": wer}"
      ],
      "metadata": {
        "id": "IAjO2q2hf9uX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "XTasK68IitHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
        "# def data_collator(features):\n",
        "#     input_values = [f[\"input_values\"] for f in features]\n",
        "#     labels = [f[\"labels\"] for f in features]\n",
        "#     batch = processor.pad({\"input_values\": input_values, \"labels\": labels}, return_tensors=\"pt\")\n",
        "#     batch[\"labels\"] = torch.tensor([[-100 if token == processor.tokenizer.pad_token_id else token for token in label]\n",
        "#                                     for label in batch[\"labels\"]])\n"
      ],
      "metadata": {
        "id": "8D3YAVXSlHTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "   output_dir=\"./test\",\n",
        "   per_device_train_batch_size=8,\n",
        "   per_device_eval_batch_size=8,\n",
        "  #  eval_strategy=\"steps\",\n",
        "  #  eval_steps=10,                 # Валидация каждые 10 шагов\n",
        "   save_steps=10,                 # Сохранение каждые 10 шагов\n",
        "   logging_steps=1,              # Логи каждые 10 шагов\n",
        "   max_steps=100,                 # Ровно 100 шагов\n",
        "   learning_rate=1e-6,\n",
        "   weight_decay=0.005,\n",
        "   warmup_steps=20,               # 20% от 100 шагов\n",
        "   save_total_limit=2,\n",
        "   metric_for_best_model=\"eval_loss\",\n",
        "   greater_is_better=False,       # Меньший loss = лучше\n",
        ")"
      ],
      "metadata": {
        "id": "kdAWVAVGgAtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    data_collator=data_collator,\n",
        "    args=training_args,\n",
        "    # compute_metrics=compute_metrics,\n",
        "    train_dataset=dataset,\n",
        "    # eval_dataset=test_dataset,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")"
      ],
      "metadata": {
        "id": "P0mCAwhpgF9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "juEFti2AgKhx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}