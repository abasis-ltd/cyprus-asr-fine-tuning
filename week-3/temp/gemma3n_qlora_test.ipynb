{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1af77912-2e2a-4c2b-9572-1f1b0f1d92fe",
      "metadata": {
        "id": "1af77912-2e2a-4c2b-9572-1f1b0f1d92fe"
      },
      "outputs": [],
      "source": [
        "# !pip install ipywidgets # huggin face widgets\n",
        "# !pip install --upgrade timm # timm error gpu gemma 3n\n",
        "# !pip install torchcodec\n",
        "# !pip install librosa soundfile\n",
        "\n",
        "## audio errors\n",
        "# !sudo apt update\n",
        "# !sudo apt install -y ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f557328-fa96-4a10-9ddb-16e4a786675b",
      "metadata": {
        "id": "4f557328-fa96-4a10-9ddb-16e4a786675b"
      },
      "outputs": [],
      "source": [
        "# ##############################\n",
        "# #Memory cleaning\n",
        "\n",
        "# import torch\n",
        "# import gc\n",
        "\n",
        "# torch.cuda.empty_cache()\n",
        "# gc.collect() # python garbage collector\n",
        "# ##############################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "becb7d4b-f276-4053-9575-c02cc8d4d6a7",
      "metadata": {
        "id": "becb7d4b-f276-4053-9575-c02cc8d4d6a7"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62ef3c89-ac05-4116-b2c3-2ec5dd3c2da9",
      "metadata": {
        "id": "62ef3c89-ac05-4116-b2c3-2ec5dd3c2da9"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "cypriot_audio = load_dataset(\"Elormiden/MilaMou_Cypriot_Dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e23d9dd4-65db-4ec9-aab5-53ef2a08b6ce",
      "metadata": {
        "id": "e23d9dd4-65db-4ec9-aab5-53ef2a08b6ce"
      },
      "outputs": [],
      "source": [
        "cypriot_audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c02d0de-6e45-4129-9cda-e601374511d5",
      "metadata": {
        "id": "3c02d0de-6e45-4129-9cda-e601374511d5"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "def fix_audio_dataset(dataset):\n",
        "    def process_audio(example):\n",
        "        try:\n",
        "            audio_decoder = example['audio']\n",
        "            if hasattr(audio_decoder, 'path'):\n",
        "                audio_array, sr = librosa.load(audio_decoder.path, sr=16000)\n",
        "            else:\n",
        "                audio_array = np.zeros(16000)\n",
        "                sr = 16000\n",
        "            example['audio'] = {\n",
        "                'array': audio_array.astype(np.float32),\n",
        "                'sampling_rate': sr\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка: {e}\")\n",
        "            example['audio'] = {\n",
        "                'array': np.zeros(16000, dtype=np.float32),\n",
        "                'sampling_rate': 16000\n",
        "            }\n",
        "        return example\n",
        "    fixed = {}\n",
        "    for split_name, split_data in dataset.items():\n",
        "        print(f\"Обрабатываю {split_name}...\")\n",
        "        fixed[split_name] = split_data.map(process_audio, num_proc=1)\n",
        "\n",
        "    return datasets.DatasetDict(fixed)\n",
        "\n",
        "fixed_dataset = fix_audio_dataset(cypriot_audio)\n",
        "sample = fixed_dataset['train'][0]\n",
        "print(f\"Audio type: {type(sample['audio'])}\")\n",
        "print(f\"Audio shape: {sample['audio']['array'].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b33b6609-b5b2-474a-b595-61bf0e647ddc",
      "metadata": {
        "id": "b33b6609-b5b2-474a-b595-61bf0e647ddc"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoProcessor,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    Gemma3nForConditionalGeneration,\n",
        "    Gemma3nProcessor,\n",
        "    TrainingArguments,\n",
        "    AutoModelForImageTextToText,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig,\n",
        "    )\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import safetensors.torch\n",
        "from datasets import Dataset\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "\n",
        "import requests\n",
        "from PIL import Image\n",
        "import librosa\n",
        "from io import BytesIO\n",
        "import logging\n",
        "from typing import Union, Tuple\n",
        "from dataclasses import dataclass\n",
        "import os\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "warnings.filterwarnings('ignore')\n",
        "logging.getLogger(\"pyngrok\").setLevel(logging.ERROR)\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "logging.getLogger(\"torch\").setLevel(logging.ERROR)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f17a835-26d8-489b-bfbc-45d53c4d1a52",
      "metadata": {
        "id": "9f17a835-26d8-489b-bfbc-45d53c4d1a52"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import mlflow\n",
        "from pyngrok import ngrok\n",
        "import sys\n",
        "\n",
        "wandb_token = \"\"\n",
        "ngrok_token = \"\"\n",
        "\n",
        "ngrok.set_auth_token(ngrok_token)\n",
        "port = \"5000\"\n",
        "\n",
        "mlflow_proc = subprocess.Popen([\n",
        "    sys.executable, \"-m\", \"mlflow\", \"ui\", \"--port\", port\n",
        "])\n",
        "\n",
        "mlflow.autolog()\n",
        "\n",
        "public_url = ngrok.connect(port)\n",
        "print(f\"MLflow UI: {public_url}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4b8c835-5faa-4d36-a7bf-dfd616da03c0",
      "metadata": {
        "id": "a4b8c835-5faa-4d36-a7bf-dfd616da03c0"
      },
      "outputs": [],
      "source": [
        "# Configuration class\n",
        "\n",
        "GEMMA_PATH = \"google/gemma-3n-e2b-it\"\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    # Gemma3n model configuration\n",
        "    MODEL_NAME: str = GEMMA_PATH\n",
        "\n",
        "    LOAD_4_BIT: bool = True\n",
        "\n",
        "    # Generation parameters\n",
        "    MAX_NEW_TOKENS: int = 512\n",
        "\n",
        "    # Device configuration\n",
        "    TORCH_DTYPE: str = torch.bfloat16\n",
        "    DEVICE_MAP: str = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Image preprocessing\n",
        "    IMAGE_SIZE: int = 512\n",
        "\n",
        "    # Hugging Face token (if needed)\n",
        "    HF_TOKEN: str = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "628d1ee8-e04b-419d-a9a0-aff46b87cf9b",
      "metadata": {
        "id": "628d1ee8-e04b-419d-a9a0-aff46b87cf9b"
      },
      "outputs": [],
      "source": [
        "config = Config()\n",
        "print(f\"Model: {config.MODEL_NAME}\")\n",
        "print(f\"Device: {config.DEVICE_MAP}\")\n",
        "print(f\"4_BIT: {config.LOAD_4_BIT}\")\n",
        "print(f\"Data type: {config.TORCH_DTYPE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "399b755e-24f2-4543-95ad-2e2ced02fbb2",
      "metadata": {
        "id": "399b755e-24f2-4543-95ad-2e2ced02fbb2"
      },
      "outputs": [],
      "source": [
        "model = Gemma3nForConditionalGeneration.from_pretrained(\n",
        "    config.MODEL_NAME,\n",
        "    torch_dtype=config.TORCH_DTYPE, # bfloat16 does not work, float16 does not work either, only float32\n",
        "    device_map=config.DEVICE_MAP,\n",
        ")\n",
        "processor = Gemma3nProcessor.from_pretrained(config.MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73a9c0a9-c861-4e81-817c-04125a15d33e",
      "metadata": {
        "id": "73a9c0a9-c861-4e81-817c-04125a15d33e"
      },
      "outputs": [],
      "source": [
        "class LoraTrainerPipeline:\n",
        "    def __init__(self, model, processor, dataset, output_dir=\"./gemma-3n-qlora-fine-tuned-steps\"):\n",
        "        self.base_model = model\n",
        "        self.processor = processor\n",
        "        self.dataset = dataset\n",
        "        self.output_dir = output_dir\n",
        "\n",
        "        self.lora_model = None\n",
        "        self.trainer = None\n",
        "\n",
        "        self.train_dataset = dataset['train']\n",
        "        self.val_dataset = dataset['validation']\n",
        "\n",
        "    def create_audio_collator(self):\n",
        "        def audio_data_collator(features):\n",
        "            audio_arrays = []\n",
        "            texts = []\n",
        "\n",
        "            for feature in features:\n",
        "                audio_data = feature['audio']\n",
        "                if isinstance(audio_data, dict) and 'array' in audio_data:\n",
        "                    audio_array = audio_data['array']\n",
        "                    if audio_data.get('sampling_rate', 16000) != 16000:\n",
        "                        import librosa\n",
        "                        audio_array = librosa.resample(\n",
        "                            audio_array,\n",
        "                            orig_sr=audio_data['sampling_rate'],\n",
        "                            target_sr=16000\n",
        "                        )\n",
        "                else:\n",
        "                    audio_array = audio_data\n",
        "\n",
        "                audio_arrays.append(audio_array)\n",
        "                messages = [\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": [\n",
        "                            {\"type\": \"audio\", \"audio\": audio_array},\n",
        "                            {\"type\": \"text\", \"text\": \"Please transcribe this audio.\"}\n",
        "                        ]\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"content\": [\n",
        "                            {\"type\": \"text\", \"text\": feature[\"sentence\"]}\n",
        "                        ]\n",
        "                    }\n",
        "                ]\n",
        "                text = self.processor.apply_chat_template(\n",
        "                    messages,\n",
        "                    tokenize=False,\n",
        "                    add_generation_prompt=False\n",
        "                )\n",
        "                texts.append(text)\n",
        "            batch = self.processor(\n",
        "                text=texts,\n",
        "                audio=audio_arrays,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                sampling_rate=16000\n",
        "            )\n",
        "            labels = batch[\"input_ids\"].clone()\n",
        "            labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
        "\n",
        "            if hasattr(self.processor.tokenizer, 'boa_token_id'):\n",
        "                labels[labels == self.processor.tokenizer.boa_token_id] = -100\n",
        "            if hasattr(self.processor.tokenizer, 'eoa_token_id'):\n",
        "                labels[labels == self.processor.tokenizer.eoa_token_id] = -100\n",
        "            if hasattr(self.processor.tokenizer, 'audio_token_id'):\n",
        "                labels[labels == self.processor.tokenizer.audio_token_id] = -100\n",
        "\n",
        "            if hasattr(self.processor.tokenizer, 'boi_token_id'):\n",
        "                labels[labels == self.processor.tokenizer.boi_token_id] = -100\n",
        "            if hasattr(self.processor.tokenizer, 'eoi_token_id'):\n",
        "                labels[labels == self.processor.tokenizer.eoi_token_id] = -100\n",
        "            if hasattr(self.processor.tokenizer, 'image_token_id'):\n",
        "                labels[labels == self.processor.tokenizer.image_token_id] = -100\n",
        "\n",
        "            batch[\"labels\"] = labels\n",
        "\n",
        "            print(f\"Batch keys: {batch.keys()}\")\n",
        "            print(f\"input_ids shape: {batch['input_ids'].shape}\")\n",
        "            if 'input_features' in batch:\n",
        "                print(f\"input_features shape: {batch['input_features'].shape}\")\n",
        "            if 'input_features_mask' in batch:\n",
        "                print(f\"input_features_mask shape: {batch['input_features_mask'].shape}\")\n",
        "\n",
        "            return batch\n",
        "\n",
        "        return audio_data_collator\n",
        "\n",
        "    def lora_training(self):\n",
        "        model = prepare_model_for_kbit_training(self.base_model)\n",
        "        lora_config = LoraConfig(\n",
        "            r=16,\n",
        "            lora_alpha=32,\n",
        "            target_modules=[\n",
        "                # Text/Language модули (основные)\n",
        "                \"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\",\n",
        "                \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "                # Можно добавить модули для аудио энкодера если нужно\n",
        "                # \"audio_encoder.layers.*.attention.self.query\",\n",
        "                # \"audio_encoder.layers.*.attention.self.key\",\n",
        "                # \"audio_encoder.layers.*.attention.self.value\",\n",
        "            ],\n",
        "            lora_dropout=0.05,\n",
        "            bias=\"none\",\n",
        "            task_type=\"CAUSAL_LM\"\n",
        "        )\n",
        "\n",
        "        self.lora_model = get_peft_model(model, lora_config)\n",
        "        self.lora_model.print_trainable_parameters()\n",
        "\n",
        "    def model_train(self, max_steps=1000, batch_size=4, learning_rate=1e-5):\n",
        "        self.lora_training()\n",
        "        data_collator = self.create_audio_collator()\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=self.output_dir,\n",
        "            overwrite_output_dir=True,\n",
        "            max_steps=max_steps,\n",
        "            per_device_train_batch_size=batch_size,\n",
        "            gradient_accumulation_steps=8,\n",
        "            save_steps=100,\n",
        "            save_total_limit=2,\n",
        "            remove_unused_columns=False,\n",
        "            dataloader_pin_memory=False,\n",
        "            prediction_loss_only=True,\n",
        "            #####################\n",
        "            fp16=True,\n",
        "            gradient_checkpointing=True,\n",
        "            #####################\n",
        "            learning_rate=learning_rate,\n",
        "            warmup_steps=50,\n",
        "            logging_steps=10,\n",
        "            eval_strategy=\"steps\",\n",
        "            eval_steps=50,\n",
        "            dataloader_num_workers=0,\n",
        "        )\n",
        "\n",
        "        self.trainer = Trainer(\n",
        "            model=self.lora_model,\n",
        "            args=training_args,\n",
        "            train_dataset=self.train_dataset,\n",
        "            eval_dataset=self.val_dataset,\n",
        "            tokenizer=self.processor.tokenizer,\n",
        "            data_collator=data_collator,\n",
        "        )\n",
        "\n",
        "        class LoggingCallback(TrainerCallback):\n",
        "            def on_log(self, args, state, control, model=None, logs=None, **kwargs):\n",
        "                if logs:\n",
        "                    print(f\"Step {state.global_step}: {logs}\")\n",
        "\n",
        "        self.trainer.add_callback(LoggingCallback())\n",
        "\n",
        "        try:\n",
        "            self.trainer.train()\n",
        "        except Exception as e:\n",
        "            print(f\"Training error: {e}\")\n",
        "            self.trainer.save_model(f\"{self.output_dir}/emergency_checkpoint\")\n",
        "            raise\n",
        "\n",
        "    def merge_and_unload(self, checkpoint_path=None):\n",
        "        print(\"Merging LoRA and unloading PEFT weights...\")\n",
        "        if checkpoint_path:\n",
        "            self.lora_model = PeftModel.from_pretrained(self.lora_model, checkpoint_path)\n",
        "        merged_model = self.lora_model.merge_and_unload()\n",
        "        return merged_model\n",
        "\n",
        "def debug_dataset_sample(dataset, processor, index=0):\n",
        "    sample = dataset[index]\n",
        "    print(f\"Sample keys: {sample.keys()}\")\n",
        "    print(f\"Audio type: {type(sample['audio'])}\")\n",
        "    if isinstance(sample['audio'], dict):\n",
        "        print(f\"Audio keys: {sample['audio'].keys()}\")\n",
        "        print(f\"Audio array shape: {sample['audio']['array'].shape}\")\n",
        "        print(f\"Sampling rate: {sample['audio']['sampling_rate']}\")\n",
        "    print(f\"Sentence: {sample['sentence']}\")\n",
        "    try:\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"audio\", \"audio\": sample['audio']['array']},\n",
        "                    {\"type\": \"text\", \"text\": \"Please transcribe this audio.\"}\n",
        "                ]\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": sample[\"sentence\"]}\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        text = processor.apply_chat_template(messages, tokenize=False)\n",
        "        print(f\"Generated text template:\\n{text}\")\n",
        "\n",
        "        batch = processor(\n",
        "            text=[text],\n",
        "            audio=[sample['audio']['array']],\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            sampling_rate=16000\n",
        "        )\n",
        "        print(f\"Processed batch keys: {batch.keys()}\")\n",
        "        for key, value in batch.items():\n",
        "            if hasattr(value, 'shape'):\n",
        "                print(f\"{key} shape: {value.shape}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Processing error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4edc20aa-e05d-4315-bd92-8447d0fdf49b",
      "metadata": {
        "id": "4edc20aa-e05d-4315-bd92-8447d0fdf49b"
      },
      "outputs": [],
      "source": [
        "trainer_pipeline = LoraTrainerPipeline(\n",
        "    model=model,\n",
        "    processor=processor,\n",
        "    dataset=cypriot_audio\n",
        ")\n",
        "\n",
        "max_steps = 100\n",
        "trainer_pipeline.model_train(max_steps=max_steps)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}