{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GTfzYNjKCmH"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets==3.6.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import torch\n",
        "import warnings\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "warnings.filterwarnings('ignore')\n",
        "logging.getLogger(\"pyngrok\").setLevel(logging.ERROR)\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "logging.getLogger(\"torch\").setLevel(logging.ERROR)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "7udZPLyUKRRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using device: {device}')\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "JXJKiQmdKgTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoProcessor,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForMaskedLM,\n",
        "    Trainer,\n",
        "    Wav2Vec2Processor,\n",
        "    Wav2Vec2ForCTC,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding\n",
        "    )\n",
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "import re\n",
        "from datasets import load_dataset, Audio, Dataset\n",
        "import jiwer\n",
        "import zipfile\n",
        "from jiwer import wer,cer"
      ],
      "metadata": {
        "id": "2YLf4ml0Kjrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds_cy = load_dataset(\"Elormiden/Thesaurus-Cypriot-Greek-Dialect\")"
      ],
      "metadata": {
        "id": "eHxTXAWeKrdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### Old model #######\n",
        "old_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-multilingual-cased\")\n",
        "old_model = AutoModelForMaskedLM.from_pretrained(\"google-bert/bert-base-multilingual-cased\")\n",
        "#######################\n",
        "\n",
        "checkpoint_path = '/content/cypriot_bert_checkpoint_2000.zip'\n",
        "\n",
        "#### Loading fine tuned model from checkponts ####\n",
        "# with zipfile.ZipFile(checkpoint_path, 'r') as zip_ref:\n",
        "#     zip_ref.extractall('/content/checkpoint_2000')\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('/content/checkpoint_2000')\n",
        "model = AutoModelForMaskedLM.from_pretrained('/content/checkpoint_2000')"
      ],
      "metadata": {
        "id": "OAYjCcOiMMxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(tokenizer)\n",
        "# print(model)"
      ],
      "metadata": {
        "id": "kRLkxlQtTdlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_cy = ds_cy['train']\n",
        "val_cy = ds_cy['validation']"
      ],
      "metadata": {
        "id": "hr9jDV9LLHx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_heavy_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    pred_ids = np.argmax(predictions, axis=-1)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "\n",
        "    pred_texts = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    transformation = jiwer.Compose([\n",
        "        jiwer.ToLowerCase(),\n",
        "        jiwer.RemovePunctuation(),\n",
        "        jiwer.RemoveMultipleSpaces(),\n",
        "        jiwer.Strip()\n",
        "    ])\n",
        "\n",
        "    wer_scores = []\n",
        "    cer_scores = []\n",
        "\n",
        "    for pred, truth in zip(pred_texts, label_texts):\n",
        "        norm_pred = transformation(pred)\n",
        "        norm_truth = transformation(truth)\n",
        "\n",
        "        if norm_truth.strip():\n",
        "            wer_score = wer(norm_truth, norm_pred)\n",
        "            cer_score = cer(norm_truth, norm_pred)\n",
        "        else:\n",
        "            wer_score = 1.0\n",
        "            cer_score = 1.0\n",
        "\n",
        "        wer_scores.append(wer_score)\n",
        "        cer_scores.append(cer_score)\n",
        "\n",
        "    return {\n",
        "        \"wer\": np.mean(wer_scores),\n",
        "        \"cer\": np.mean(cer_scores),\n",
        "        \"wer_std\": np.std(wer_scores),\n",
        "        \"cer_std\": np.std(cer_scores)\n",
        "    }"
      ],
      "metadata": {
        "id": "W5gCrq-6LdFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "\"\"\"\n",
        "Converting text into tokens\n",
        "\"hello world\" → [101, 1234, 5678, 102]\n",
        "\"\"\"\n",
        "def tokenize_multilingual_text(text):\n",
        "    ids = tokenizer(text, max_length=256, truncation=True)['input_ids']\n",
        "    return ids\n",
        "\n",
        "\"\"\"\n",
        "1. Creating of prompts:\n",
        "input_text = f\"correct cypriot to greek: {word} - {description}\"\n",
        "target_text = f\"{greek_word} - {greek_desc}\"\n",
        "2. Tokenization of the whole batch\n",
        "3. Converting PyTorch list into tensors\n",
        "4. Texts has different length, make the equal padding\n",
        "5. Converting 0 to -100, saying to a model not to count these positions\n",
        "\"\"\"\n",
        "def tokenize_text_pairs(batch):\n",
        "    tokenized_input_texts = []\n",
        "    tokenized_target_texts = []\n",
        "\n",
        "    for word, description, greek_word, greek_desc in tqdm(\n",
        "        zip(batch['word'], batch['description'], batch['greek_word'], batch['greek_description']),\n",
        "        total=len(batch['word']), desc=\"Tokenizing batch\"):\n",
        "\n",
        "        input_text = f\"correct cypriot to greek: {word} - {description}\"\n",
        "        target_text = f\"{greek_word} - {greek_desc}\"\n",
        "\n",
        "        tokenized_input_texts.append(tokenize_multilingual_text(input_text))\n",
        "        tokenized_target_texts.append(tokenize_multilingual_text(target_text))\n",
        "\n",
        "    input_ids_tensors = [torch.tensor(ids, dtype=torch.long) for ids in tokenized_input_texts]\n",
        "    labels_tensors = [torch.tensor(ids, dtype=torch.long) for ids in tokenized_target_texts]\n",
        "\n",
        "    input_ids_padded = pad_sequence(input_ids_tensors, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    labels_padded = pad_sequence(labels_tensors, batch_first=True, padding_value=-100)\n",
        "\n",
        "    attention_mask = (input_ids_padded != tokenizer.pad_token_id).long()\n",
        "    labels_padded[labels_padded == tokenizer.pad_token_id] = -100\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids_padded,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels_padded\n",
        "    }"
      ],
      "metadata": {
        "id": "ln-MTfI2Lm1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Filling into the function\n",
        "\"\"\"\n",
        "train_cyprus_tokenized = tokenize_text_pairs(train_cy)\n",
        "val_cyprus_tokenized = tokenize_text_pairs(val_cy)"
      ],
      "metadata": {
        "id": "q4V_94CVT6Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Converting dicts to Dataset HuggingFace format\n",
        "\"\"\"\n",
        "train_hf = Dataset.from_dict(train_cyprus_tokenized)\n",
        "val_hf = Dataset.from_dict(val_cyprus_tokenized)"
      ],
      "metadata": {
        "id": "J9-fGaLrT7dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_hf"
      ],
      "metadata": {
        "id": "cyY1ctzLdJyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Function does not work with this MLM, will explore another approach\n",
        "\n",
        "1. Model goes to eval mode, gpu/cpu tracking device\n",
        "2. Evaluation by batches, 4 batches at the time\n",
        "3. Tokens -> Tensors\n",
        "4. Getting predictions from a model\n",
        "5. Decode to text\n",
        "6. Metrics counter\n",
        "\"\"\"\n",
        "\n",
        "def calculate_wer_cer_separately(model, tokenizer, eval_dataset, batch_size=4, max_samples=50):\n",
        "   model.eval()\n",
        "   predictions = []\n",
        "   references = []\n",
        "\n",
        "   device = model.device\n",
        "\n",
        "   print(f\"Исходный размер датасета: {len(eval_dataset)}\")\n",
        "   eval_subset = eval_dataset.select(range(min(max_samples, len(eval_dataset))))\n",
        "   print(f\"Samples {len(eval_subset)} taken\")\n",
        "   print(\"-\"*50)\n",
        "\n",
        "   total_batches = len(eval_subset) // batch_size + (1 if len(eval_subset) % batch_size != 0 else 0)\n",
        "\n",
        "   for i in tqdm(range(0, len(eval_subset), batch_size),\n",
        "                 desc=\"Calculating WER/CER\",\n",
        "                 total=total_batches):\n",
        "       batch = eval_subset[i:i+batch_size]\n",
        "\n",
        "       input_ids = torch.stack([torch.tensor(x) for x in batch['input_ids']]).to(device)\n",
        "       labels = torch.stack([torch.tensor(x) for x in batch['labels']]).to(device)\n",
        "       attention_mask = torch.stack([torch.tensor(x) for x in batch['attention_mask']]).to(device)\n",
        "\n",
        "       with torch.no_grad():\n",
        "           outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "           pred_ids = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "       for pred, label in zip(pred_ids, labels):\n",
        "           label_clean = label[label != -100]\n",
        "           pred_clean = pred[:len(label_clean)]\n",
        "\n",
        "           pred_text = tokenizer.decode(pred_clean, skip_special_tokens=True)\n",
        "           ref_text = tokenizer.decode(label_clean, skip_special_tokens=True)\n",
        "\n",
        "           predictions.append(pred_text)\n",
        "           references.append(ref_text)\n",
        "\n",
        "   print(\"=\"*50)\n",
        "   print(\"ОТЛАДОЧНАЯ ИНФОРМАЦИЯ\")\n",
        "   print(\"=\"*50)\n",
        "   print(f\"Обработано примеров: {len(predictions)}\")\n",
        "\n",
        "   for i in range(min(10, len(predictions))):\n",
        "       print(f\"\\nПример {i+1}:\")\n",
        "       print(f\"  Предсказание: '{predictions[i]}'\")\n",
        "       print(f\"  Правильный:   '{references[i]}'\")\n",
        "       print(f\"  Совпадение:   {predictions[i] == references[i]}\")\n",
        "\n",
        "   wer_score = jiwer.wer(references, predictions)\n",
        "   cer_score = jiwer.cer(references, predictions)\n",
        "\n",
        "   print(\"=\"*50)\n",
        "   print(f\"WER: {wer_score:.3f} ({wer_score*100:.1f}% неправильных слов)\")\n",
        "   print(f\"CER: {cer_score:.3f} ({cer_score*100:.1f}% неправильных символов)\")\n",
        "   print(\"=\"*50)\n",
        "\n",
        "   return {\n",
        "       \"wer\": wer_score,\n",
        "       \"cer\": cer_score,\n",
        "       \"predictions\": predictions[:5],\n",
        "       \"references\": references[:5]\n",
        "   }"
      ],
      "metadata": {
        "id": "W3ym4-M4V9FO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# results = calculate_wer_cer_separately(model, tokenizer, val_hf, batch_size=1, max_samples=50)\n",
        "# old_results = calculate_wer_cer_separately(old_model, old_tokenizer, val_hf, batch_size=4)"
      ],
      "metadata": {
        "id": "s-R5rRb6XYu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############ 2000 Steps Fine-tuned model results ############\n",
        "print(\"NEW MODEL\")\n",
        "print(f\"WER: {results['wer']:.3f}\")\n",
        "print(f\"CER: {results['cer']:.3f}\")\n",
        "#############################################################\n",
        "\n",
        "############# Clean/Old model results ######################\n",
        "# print(\"OLD MODEL:\")\n",
        "# print(f\"WER: {old_results['wer']:.3f}\")\n",
        "# print(f\"CER: {old_results['cer']:.3f}\")\n",
        "############################################################"
      ],
      "metadata": {
        "id": "NzuoXs2xY2H-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# eval_args = TrainingArguments(\n",
        "#     output_dir=\"./evaluation_results\",\n",
        "\n",
        "#     per_device_eval_batch_size=1,\n",
        "\n",
        "#     fp16=True,\n",
        "#     dataloader_num_workers=0,\n",
        "#     dataloader_pin_memory=False,\n",
        "\n",
        "#     remove_unused_columns=True,\n",
        "#     logging_steps=50,\n",
        "#     report_to=[],\n",
        "\n",
        "#     save_strategy=\"no\",\n",
        "#     eval_strategy=\"no\",\n",
        "# )\n",
        "\n",
        "# evaluator = Trainer(\n",
        "#     model=model,\n",
        "#     args=eval_args,\n",
        "#     eval_dataset=val_hf,\n",
        "#     compute_metrics=compute_heavy_metrics,\n",
        "#     tokenizer=tokenizer,\n",
        "# )\n",
        "\n",
        "# torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Y94XZkc3UOps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# eval_results = evaluator.evaluate()"
      ],
      "metadata": {
        "id": "3x8XWQfVUVR9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}