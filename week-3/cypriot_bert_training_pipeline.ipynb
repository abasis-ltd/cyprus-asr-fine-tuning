{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h96mnJkxApwF"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets==3.6.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import torch\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm import tqdm\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "warnings.filterwarnings('ignore')\n",
        "logging.getLogger(\"pyngrok\").setLevel(logging.ERROR)\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "logging.getLogger(\"torch\").setLevel(logging.ERROR)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "wiBKLukoBC9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using device: {device}')\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "id": "MS8_SWlHBEXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset, Audio\n",
        "\n",
        "ds_cy = load_dataset(\"Elormiden/Thesaurus-Cypriot-Greek-Dialect\")"
      ],
      "metadata": {
        "id": "UG3MfxypBFtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-multilingual-cased\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"google-bert/bert-base-multilingual-cased\")"
      ],
      "metadata": {
        "id": "6pu4h3ujBJ-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Dividing our initial dataset by columns\n",
        "\"\"\"\n",
        "train_cy = ds_cy['train']\n",
        "val_cy = ds_cy['validation']"
      ],
      "metadata": {
        "id": "ZlLUZRy9BcXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "\"\"\"\n",
        "Converting text into tokens\n",
        "\"hello world\" â†’ [101, 1234, 5678, 102]\n",
        "\"\"\"\n",
        "def tokenize_multilingual_text(text):\n",
        "    ids = tokenizer(text, max_length=256, truncation=True)['input_ids']\n",
        "    return ids\n",
        "\n",
        "\"\"\"\n",
        "1. Creating of prompts:\n",
        "input_text = f\"correct cypriot to greek: {word} - {description}\"\n",
        "target_text = f\"{greek_word} - {greek_desc}\"\n",
        "2. Tokenization of the whole batch\n",
        "3. Converting PyTorch list into tensors\n",
        "4. Texts has different length, make the equal padding\n",
        "5. Converting 0 to -100, saying to a model not to count these positions\n",
        "\"\"\"\n",
        "def tokenize_text_pairs(batch):\n",
        "    tokenized_input_texts = []\n",
        "    tokenized_target_texts = []\n",
        "\n",
        "    for word, description, greek_word, greek_desc in tqdm(\n",
        "        zip(batch['word'], batch['description'], batch['greek_word'], batch['greek_description']),\n",
        "        total=len(batch['word']), desc=\"Tokenizing batch\"):\n",
        "\n",
        "        input_text = f\"correct cypriot to greek: {word} - {description}\"\n",
        "        target_text = f\"{greek_word} - {greek_desc}\"\n",
        "\n",
        "        tokenized_input_texts.append(tokenize_multilingual_text(input_text))\n",
        "        tokenized_target_texts.append(tokenize_multilingual_text(target_text))\n",
        "\n",
        "    input_ids_tensors = [torch.tensor(ids, dtype=torch.long) for ids in tokenized_input_texts]\n",
        "    labels_tensors = [torch.tensor(ids, dtype=torch.long) for ids in tokenized_target_texts]\n",
        "\n",
        "    input_ids_padded = pad_sequence(input_ids_tensors, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    labels_padded = pad_sequence(labels_tensors, batch_first=True, padding_value=-100)\n",
        "\n",
        "    attention_mask = (input_ids_padded != tokenizer.pad_token_id).long()\n",
        "    labels_padded[labels_padded == tokenizer.pad_token_id] = -100\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids_padded,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels_padded\n",
        "    }"
      ],
      "metadata": {
        "id": "z5TC3Dt6ChKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Filling into the function\n",
        "\"\"\"\n",
        "train_cyprus_tokenized = tokenize_text_pairs(train_cy)\n",
        "val_cyprus_tokenized = tokenize_text_pairs(val_cy)"
      ],
      "metadata": {
        "id": "ci2gf1ZKEBvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Converting dicts to Dataset HuggingFace format\n",
        "\"\"\"\n",
        "train_hf = Dataset.from_dict(train_cyprus_tokenized)\n",
        "val_hf = Dataset.from_dict(val_cyprus_tokenized)"
      ],
      "metadata": {
        "id": "FFIN0tTaFaqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./cypriot-corrector-bert\",\n",
        "    num_train_epochs=8,\n",
        "    ################# ~ 10gb of GPU\n",
        "    per_device_train_batch_size=24,\n",
        "    per_device_eval_batch_size=20,\n",
        "    gradient_accumulation_steps=2,\n",
        "    ################\n",
        "    learning_rate=5e-4,\n",
        "    warmup_steps=500,\n",
        "    #################### GPU, eat less memory\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    #################\n",
        "    save_steps=200,\n",
        "    eval_steps=50,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        "    report_to='wandb',\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_hf,\n",
        "    eval_dataset=val_hf,\n",
        "    # compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "I3uL0BYMEySe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "EL1ZXXrKE9mX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}