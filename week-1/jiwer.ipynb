{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqIfABDpbw2a"
      },
      "outputs": [],
      "source": [
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LINTERS\n",
        "###############################\n",
        "# !nbqa mypy jiwer.ipynb\n",
        "# !nbqa black jiwer.ipynb\n",
        "# !nbqa flake8 jiwer.ipynb\n",
        "###############################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vJWjK3xsL50"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import mlflow # mlFlow\n",
        "from pyngrok import ngrok # workaround for localhost\n",
        "\n",
        "wandb_token = os.getenv(\"WANDB_TOKEN\")\n",
        "ngrok_token = os.getenv(\"NGROK_TOKEN\")\n",
        "\n",
        "# https://dashboard.ngrok.com/authtokens\n",
        "ngrok.set_auth_token(ngrok_token)\n",
        "port = \"5000\"\n",
        "\n",
        "mlflow_proc = subprocess.Popen([\"mlflow\", \"ui\", \"--port\", port])\n",
        "mlflow.autolog()\n",
        "# mlflow_proc.terminate()\n",
        "\n",
        "public_url = ngrok.connect(port)\n",
        "print(f\"MLflow UI: {public_url}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4qamzPgkvmJ"
      },
      "outputs": [],
      "source": [
        "############################\n",
        "# Whisper dependencies\n",
        "from faster_whisper import WhisperModel\n",
        "############################\n",
        "\n",
        "###############################\n",
        "# DeepGram dependencies\n",
        "import requests\n",
        "import wave\n",
        "import io\n",
        "import time\n",
        "import os\n",
        "import torch\n",
        "import logging\n",
        "import json\n",
        "import threading\n",
        "from datetime import datetime\n",
        "import deepgram\n",
        "from deepgram import (\n",
        "  DeepgramClient,\n",
        "  DeepgramClientOptions,\n",
        "  AgentWebSocketEvents,\n",
        "  AgentKeepAlive,\n",
        "  PrerecordedOptions,\n",
        "  FileSource\n",
        ")\n",
        "################################\n",
        "\n",
        "# Elevenlabs dependencies\n",
        "################################\n",
        "from elevenlabs.client import ElevenLabs\n",
        "from io import BytesIO\n",
        "import requests\n",
        "################################\n",
        "\n",
        "import assemblyai as aai\n",
        "from rev_ai import apiclient\n",
        "from openai import OpenAI\n",
        "from google.cloud import speech\n",
        "from google.oauth2 import service_account\n",
        "from openai import AzureOpenAI\n",
        "import psutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Initialize the client\n",
        "deepgram = DeepgramClient(os.getenv(\"DEEPCLIENT_TOKEN\"))\n",
        "elevenlabs_client = ElevenLabs(api_key=os.getenv(\"ELEVENLABS_TOKEN\"))\n",
        "aai.settings.api_key = os.getenv(\"ASSEMBLY_TOKEN\")\n",
        "rev_token = os.getenv(\"REV_TOKEN\")\n",
        "\n",
        "###################################\n",
        "# Mozilla Common Voice for testing\n",
        "from datasets import load_dataset, Audio\n",
        "\n",
        "# Greek dataset\n",
        "dataset = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"el\", split=\"train\")\n",
        "\n",
        "# Squeezing to 16hz\n",
        "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "###################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_GURe_FKc95"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Model Loader Pipeline\n",
        "\"\"\"\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torchaudio\n",
        "import soundfile as sf\n",
        "\n",
        "class ModelPipeline:\n",
        "  def __init__(self, model_name: str, audio: str):\n",
        "    self.model_name = model_name\n",
        "    self.audio = audio\n",
        "    self.processor = None\n",
        "    self.model = None\n",
        "\n",
        "  # Whisper\n",
        "  #####################################\n",
        "  def load_whisper_model(self) -> None:\n",
        "    self.model = WhisperModel(self.model_name)\n",
        "\n",
        "  def whisper_process_logic(self) -> str:\n",
        "    text_stored = \"\"\n",
        "    segments, _ = self.model.transcribe(self.audio, language=\"el\") # <- specify language output\n",
        "    for segment in segments:\n",
        "      # print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text)) # <- to see with seconds\n",
        "      text_stored += segment.text + \" \"\n",
        "    return text_stored\n",
        "  #####################################\n",
        "\n",
        "  # HuggingFace\n",
        "  #####################################\n",
        "  def load_hf_model(self) -> None:\n",
        "    self.model = Wav2Vec2ForCTC.from_pretrained(self.model_name)\n",
        "\n",
        "  def load_hf_processor(self) -> None:\n",
        "    self.processor = Wav2Vec2Processor.from_pretrained(self.model_name)\n",
        "\n",
        "  def hf_process_logic(self) -> str:\n",
        "    speech = self.audio[\"array\"]\n",
        "    sr = self.audio[\"sampling_rate\"]\n",
        "\n",
        "    if sr != 16000:\n",
        "        speech = librosa.resample(speech, orig_sr=sr, target_sr=16000)\n",
        "        sr = 16000\n",
        "\n",
        "    inputs = self.processor(speech, sampling_rate=sr, return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = self.model(**inputs).logits\n",
        "\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "    transcription = self.processor.decode(predicted_ids[0])\n",
        "\n",
        "    return transcription\n",
        "  #####################################\n",
        "\n",
        "  # DeepGram\n",
        "  #####################################\n",
        "  def deepgram_process_logic(self) -> str:\n",
        "        # Handle different audio input formats\n",
        "        if isinstance(self.audio, dict) and \"array\" in self.audio:\n",
        "            # HuggingFace dataset format\n",
        "            audio_array = self.audio[\"array\"]\n",
        "            sample_rate = self.audio[\"sampling_rate\"]\n",
        "        elif isinstance(self.audio, str):\n",
        "            # File path format\n",
        "            audio_array, sample_rate = sf.read(self.audio)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported audio format: {type(self.audio)}\")\n",
        "\n",
        "        # Convert to bytes buffer\n",
        "        buffer = io.BytesIO()\n",
        "        sf.write(buffer, audio_array, sample_rate, format='WAV')\n",
        "        buffer.seek(0)  # Reset to the beginning\n",
        "\n",
        "        # Configure options\n",
        "        options = PrerecordedOptions(\n",
        "            model=\"nova-2\",\n",
        "            smart_format=True,\n",
        "            language=\"el\",\n",
        "        )\n",
        "\n",
        "        # Transcribe using buffer data\n",
        "        response = deepgram.listen.rest.v(\"1\").transcribe_file(\n",
        "            {\"buffer\": buffer, \"mimetype\": \"audio/wav\"},\n",
        "            options\n",
        "        )\n",
        "        transcript = response[\"results\"][\"channels\"][0][\"alternatives\"][0][\"transcript\"]\n",
        "\n",
        "        return transcript\n",
        "  #####################################\n",
        "\n",
        "  # Elevenlabs\n",
        "  #####################################\n",
        "  def eleven_process_logic(self) -> str:\n",
        "    # Check if self.audio is a URL or a local file path\n",
        "    if self.audio.startswith(('http://', 'https://')):\n",
        "        # It's a URL, use requests to download\n",
        "        response = requests.get(self.audio)\n",
        "        response.raise_for_status()  # Raise an exception for bad status codes\n",
        "        audio_data = BytesIO(response.content)\n",
        "    else:\n",
        "        # It's a local file path, read directly\n",
        "        try:\n",
        "            with open(self.audio, 'rb') as f:\n",
        "                audio_data = BytesIO(f.read())\n",
        "        except FileNotFoundError:\n",
        "            raise FileNotFoundError(f\"Audio file not found: {self.audio}\")\n",
        "        except PermissionError:\n",
        "            raise PermissionError(f\"Permission denied accessing audio file: {self.audio}\")\n",
        "\n",
        "    transcription = elevenlabs_client.speech_to_text.convert(\n",
        "        file=audio_data,\n",
        "        model_id=\"scribe_v1\", # Model to use, for now only \"scribe_v1\" is supported\n",
        "        tag_audio_events=True, # Tag audio events like laughter, applause, etc.\n",
        "        language_code=\"ell\", # Language of the audio file. If set to None, the model will detect the language automatically.\n",
        "        diarize=True, # Whether to annotate who is speaking\n",
        "    )\n",
        "    return transcription.text # <- there was an error with output\n",
        "  #####################################\n",
        "\n",
        "  # AssemblyAI\n",
        "  #####################################\n",
        "  def assembly_process_logic(self) -> str:\n",
        "    # audio_file = \"./local_file.mp3\"\n",
        "    # audio_file = \"https://assembly.ai/wildfires.mp3\"\n",
        "    audio_file = self.audio\n",
        "\n",
        "    config = aai.TranscriptionConfig(speech_model=aai.SpeechModel.best)\n",
        "\n",
        "    transcript = aai.Transcriber(config=config).transcribe(audio_file)\n",
        "\n",
        "    if transcript.status == \"error\":\n",
        "      raise RuntimeError(f\"Transcription failed: {transcript.error}\")\n",
        "\n",
        "    return transcript.text\n",
        "  #####################################\n",
        "\n",
        "  # Speechmatics\n",
        "  ######################################\n",
        "  def speechmatics_process_logic(self) -> str:\n",
        "    from speechmatics.models import ConnectionSettings\n",
        "    from speechmatics.batch_client import BatchClient\n",
        "    from httpx import HTTPStatusError\n",
        "\n",
        "    API_KEY = os.getenv(\"SPEECHMATICS_TOKEN\")\n",
        "    PATH_TO_FILE = self.audio\n",
        "    LANGUAGE = \"el\"\n",
        "\n",
        "    settings = ConnectionSettings(\n",
        "        url=\"https://asr.api.speechmatics.com/v2\",\n",
        "        auth_token=API_KEY,\n",
        "    )\n",
        "\n",
        "    # Define transcription parameters\n",
        "    conf = {\n",
        "        \"type\": \"transcription\",\n",
        "        \"transcription_config\": {\n",
        "            \"language\": LANGUAGE\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Open the client using a context manager\n",
        "    with BatchClient(settings) as client:\n",
        "        job_id = client.submit_job(\n",
        "            audio=PATH_TO_FILE,\n",
        "            transcription_config=conf,\n",
        "        )\n",
        "        print(f'job {job_id} submitted successfully, waiting for transcript')\n",
        "\n",
        "        # Note that in production, you should set up notifications instead of polling.\n",
        "        # Notifications are described here: https://docs.speechmatics.com/features-other/notifications\n",
        "        transcript = client.wait_for_completion(job_id, transcription_format='txt')\n",
        "        # To see the full output, try setting transcription_format='json-v2'.\n",
        "        return transcript\n",
        "\n",
        "  ######################################\n",
        "  \n",
        "  # OpenAI\n",
        "  ######################################\n",
        "  def azure_openai_process_logic(self) -> str:\n",
        "    \"\"\"\n",
        "    Process audio using Azure OpenAI transcription models.\n",
        "    \"\"\"\n",
        "    # Azure OpenAI configuration\n",
        "    client = AzureOpenAI(\n",
        "        api_key=os.getenv(\"AZURE_OPENAI_TOKEN\"),\n",
        "        api_version=\"2024-02-01\",\n",
        "        azure_endpoint=\"https://ai-mcoublm5-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-4o-transcribe/audio/transcriptions?api-version=2025-03-01-preview\"\n",
        "    )\n",
        "\n",
        "    with open(self.audio, \"rb\") as audio_file:\n",
        "        transcription = client.audio.transcriptions.create(\n",
        "            model=\"gpt-4o-transcribe\",\n",
        "            file=audio_file,\n",
        "            language=\"el\"  # <- put language here\n",
        "        )\n",
        "\n",
        "    return transcription.text\n",
        "  ######################################\n",
        "\n",
        "  # Google Cloud\n",
        "  ######################################\n",
        "  def google_cloud_process_logic(self) -> str:\n",
        "    import os\n",
        "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'google.json'\n",
        "    client = speech.SpeechClient()\n",
        "\n",
        "    with open(self.audio, \"rb\") as audio_file:\n",
        "        content = audio_file.read()\n",
        "\n",
        "    audio = speech.RecognitionAudio(content=content)\n",
        "    config = speech.RecognitionConfig(\n",
        "        encoding=speech.RecognitionConfig.AudioEncoding.MP3,\n",
        "        language_code=\"el-GR\"\n",
        "    )\n",
        "\n",
        "    response = client.recognize(config=config, audio=audio)\n",
        "\n",
        "    transcript = \"\"\n",
        "    for result in response.results:\n",
        "        transcript += result.alternatives[0].transcript + \" \"\n",
        "\n",
        "    return transcript.strip()\n",
        "  ######################################\n",
        "\n",
        "  # Google Gemini\n",
        "  ######################################\n",
        "\n",
        "  def google_gemini_process_logic(self) -> str:\n",
        "    from google import genai\n",
        "    client = genai.Client(api_key=\"\")\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.5-pro\",\n",
        "        contents=\"Explain how AI works in a few words\",\n",
        "    )\n",
        "\n",
        "    return response.text\n",
        "  ######################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiKCeXr3JR2F"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "  Jiwer Loader Pipeline\n",
        "\"\"\"\n",
        "import jiwer\n",
        "from jiwer import wer,cer\n",
        "\n",
        "\"\"\"\n",
        "wer - word error rate\n",
        "cer - character error rate\n",
        "\"\"\"\n",
        "\n",
        "class JiwerMetricsPipeline:\n",
        "  def __init__(self, hypothesis: str, truth: str):\n",
        "    self.hypothesis = hypothesis\n",
        "    self.truth = truth\n",
        "\n",
        "  def compute_metrics(self) -> tuple[float, float]:\n",
        "    transformation = jiwer.Compose([\n",
        "        jiwer.ToLowerCase(),\n",
        "        jiwer.RemovePunctuation(),\n",
        "        jiwer.RemoveMultipleSpaces(),\n",
        "        jiwer.Strip()\n",
        "    ])\n",
        "\n",
        "    normalized_truth = transformation(self.truth)\n",
        "    normalized_hypothesis = transformation(self.hypothesis)\n",
        "\n",
        "    wer_score = wer(normalized_truth, normalized_hypothesis)\n",
        "    cer_score = cer(normalized_truth, normalized_hypothesis)\n",
        "\n",
        "    return wer_score, cer_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7N4T9WdOmt-"
      },
      "outputs": [],
      "source": [
        "run_counter = 0\n",
        "\n",
        "# Metrics extensions\n",
        "from mlflow.models import infer_signature\n",
        "from mlflow.data.pandas_dataset import PandasDataset\n",
        "\n",
        "def launch_model(model: str, sentence: str, audio: str) -> None:\n",
        "  global run_counter\n",
        "  run_counter += 1\n",
        "  with mlflow.start_run(run_name=f\"{model}_audio_{run_counter}\"):\n",
        "\n",
        "    # Dataset Mlflow\n",
        "    #################################\n",
        "    dataset_info = pd.DataFrame({\n",
        "      'audio_path': [audio],\n",
        "      'truth_text': [sentence],\n",
        "      'text_length': [len(sentence)]\n",
        "    })\n",
        "\n",
        "    dataset = mlflow.data.from_pandas(\n",
        "      dataset_info,\n",
        "      source=f\"huggingface\",\n",
        "      name=\"mozilla_common_voice\",\n",
        "    )\n",
        "\n",
        "    mlflow.log_input(dataset, context=\"training\")\n",
        "    #################################\n",
        "\n",
        "    # Log params for mlflow\n",
        "    #################################\n",
        "    mlflow.log_param(\"model_name\", model)\n",
        "    mlflow.log_param(\"timestamp\", datetime.now().isoformat())\n",
        "    #################################\n",
        "\n",
        "    # Specs logging\n",
        "    #################################\n",
        "    mlflow.log_param(\"cpu_count\", psutil.cpu_count())\n",
        "    mlflow.log_param(\"memory_total_gb\", round(psutil.virtual_memory().total / 1024**3, 1))\n",
        "    #################################\n",
        "\n",
        "    # Time logging\n",
        "    ##########################\n",
        "    start_time = time.time()\n",
        "\n",
        "    llm = ModelPipeline(model, audio)\n",
        "\n",
        "    memory_before = psutil.virtual_memory().used / 1024**3 # before whisper.process_logic\n",
        "    ###############################\n",
        "\n",
        "    \"\"\"\n",
        "    whisper\n",
        "    \"\"\"\n",
        "    # llm.load_whisper_model()\n",
        "\n",
        "    # process_start = time.time()\n",
        "    # whisper_output = llm.whisper_process_logic()\n",
        "    # process_time = time.time() - process_start\n",
        "\n",
        "    \"\"\"\n",
        "    huggingface\n",
        "    \"\"\"\n",
        "    # llm.load_hf_model()\n",
        "    # llm.load_hf_processor()\n",
        "\n",
        "    # process_start = time.time()\n",
        "    # hf_output = llm.hf_process_logic()\n",
        "    # process_time = time.time() - process_start\n",
        "\n",
        "    \"\"\"\n",
        "    deepgram\n",
        "    \"\"\"\n",
        "    # process_start = time.time()\n",
        "    # llm_output = llm.deepgram_process_logic()\n",
        "    # process_time = time.time() - process_start\n",
        "\n",
        "    \"\"\"\n",
        "    elevenlabs\n",
        "    \"\"\"\n",
        "    # process_start = time.time()\n",
        "    # llm_output = llm.eleven_process_logic()\n",
        "    # process_time = time.time() - process_start\n",
        "\n",
        "    \"\"\"\n",
        "    assembly\n",
        "    \"\"\"\n",
        "    # process_start = time.time()\n",
        "    # llm_output = llm.assembly_process_logic()\n",
        "    # process_time = time.time() - process_start\n",
        "\n",
        "    \"\"\"\n",
        "    speechmatics\n",
        "    \"\"\"\n",
        "    # process_start = time.time()\n",
        "    # llm_output = llm.speechmatics_process_logic()\n",
        "    # process_time = time.time() - process_start\n",
        "\n",
        "    \"\"\"\n",
        "    azure openai\n",
        "    \"\"\"\n",
        "    # process_start = time.time()\n",
        "    # llm_output = llm.azure_openai_process_logic()\n",
        "    # process_time = time.time() - process_start\n",
        "\n",
        "    \"\"\"\n",
        "    google cloud\n",
        "    \"\"\"\n",
        "    # process_start = time.time()\n",
        "    # llm_output = llm.google_cloud_process_logic()\n",
        "    # process_time = time.time() - process_start\n",
        "\n",
        "    \"\"\"\n",
        "    google gemini\n",
        "    \"\"\"\n",
        "    process_start = time.time()\n",
        "    llm_output = llm.google_gemini_process_logic()\n",
        "    process_time = time.time() - process_start\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    memory_after = psutil.virtual_memory().used / 1024**3 # after whisper.process_logic\n",
        "\n",
        "    mlflow.log_metric(\"process_time_seconds\", round(process_time,2))\n",
        "    mlflow.log_metric(\"total_time_seconds\", total_time)\n",
        "    mlflow.log_metric(\"memory_usage_change_gb\", round(memory_after - memory_before, 3))\n",
        "    mlflow.log_metric(\"cpu_usage_percent\", psutil.cpu_percent())\n",
        "    #############################\n",
        "\n",
        "    err_metrics = JiwerMetricsPipeline(llm_output, sentence).compute_metrics()\n",
        "\n",
        "    #wer, cer\n",
        "    mlflow.log_metric(\"total_wer\", round(err_metrics[0], 2))\n",
        "    mlflow.log_metric(\"total_cer\", round(err_metrics[1], 2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vacjBFvkjrV9"
      },
      "outputs": [],
      "source": [
        "# Common launch\n",
        "for i in range(10):\n",
        "    \n",
        "    dataset_sentence = dataset[i][\"sentence\"]\n",
        "    # for whisper models and other\n",
        "    path_audio = dataset[i][\"audio\"][\"path\"]\n",
        "    # for hf models and deepgram models\n",
        "    common_audio = dataset[i][\"audio\"]\n",
        "\n",
        "    launch_model(\"model-type\", dataset_sentence, path_audio)\n",
        "    print(f\"Processed sample {i+1}/10\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyN1lpHEp5JtB36U5rWYgFVd",
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
